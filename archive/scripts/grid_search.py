"""
Grid / Walk-forward search for Order Block strategy parameters.

Usage: edit the CONFIG at the top or run as a script. It will:
- Load historical OHLCV from `binance_collector.BinanceDataCollector` if available (cached to ./data)
- Run the strategy `detect_order_blocks` from `testing.py` on train and test folds
- Compute metrics per fold and aggregate across folds
- Save full results CSV to ./results/grid_search_results_<timestamp>.csv

Notes:
- The full parameter space is large; this script supports `max_combinations` sampling
  to limit runtime. Start small and expand.

Author: generated by assistant
"""

import os
import sys
import math
import json
import time
import random
from itertools import product
from datetime import datetime
from typing import Dict, Any, List, Tuple

import pandas as pd
import numpy as np
import concurrent.futures
import matplotlib.pyplot as plt
try:
    from tqdm import tqdm
except Exception:
    tqdm = None

# Import the strategy implementation from `updated_testing.py` (the updated strategy file).
try:
    from updated_testing import detect_order_blocks
except Exception as e:
    print("Could not import detect_order_blocks from updated_testing.py:", e)
    print("Make sure updated_testing.py is in the same folder and defines detect_order_blocks(...)")
    raise

# Try to import BinanceDataCollector
try:
    from binance_collector import BinanceDataCollector
    _HAS_BINANCE = True
except Exception:
    BinanceDataCollector = None
    _HAS_BINANCE = False

# -----------------
# CONFIG
# -----------------
CONFIG = {
    # coins/timeframes to evaluate
    'coins': ['ETHUSDT'],
    'timeframes': ['1m'],
    'futures_options': [True],

    # parameter grid (user-specified)
    'grid': {
        'swing_length': [5, 10, 15, 20, 30, 50, 75, 100, 120],
        'stop_loss_multiplier': [0.5, 0.7, 1.0, 1.2, 1.5],
        'take_profit_multiplier': [2, 3, 4, 5, 6],
        'min_strength_ratio': [0.3, 0.4, 0.5, 0.6, 0.7],
        'trailing_stop_activation': [1.5, 2.0, 2.5, 3.0],
        'trailing_stop_percent': [1.5, 2.0, 2.5, 3.0],
        'violation_type': ['Wick', 'Close'],
        # OB-specific tunables
        # search window expressed as fraction of swing_length (converted to int >=1)
        'ob_search_window_frac': [0.000001, 0.25, 0.5],
        # ATR-based break multiplier (used in OB detection)
        'break_atr_mult': [0.1, 0.2],
        # SL floor tuning
    'ob_min_sl_atr_mult': [0.5, 1.0],
    'ob_min_sl_pct': [0.0005, 0.001],
        'risk_per_trade_percent': [1, 2, 3, 5]
    },

    # params kept fixed
    'fixed': {
        'trailing_stop_buffer_candles': 3,
        'trailing_stop_update_threshold': 0.5,
        'commission_percent': 0.01,
        'slippage_percent': 0.05,
        'entry_price_mode': 'Close',
        'initial_capital': 10000.0,
        'use_fixed_capital': True,
        'max_concurrent_positions': 2
        ,
        # Order-block related defaults (can be tuned via grid)
        'break_atr_mult': 0.2,
        'ob_min_sl_atr_mult': 0.5,
        'ob_min_sl_pct': 0.001
    },

    # walk-forward folds specification (list of (train_start, train_end, test_start, test_end))
    # The user asked for a two-year style: Nov->Aug train, Aug->Oct test for both 2024 and 2025
    'folds': [
        # year 2024 fold
        {
            'train_start': '2023-11-01', 'train_end': '2024-08-31',
            'test_start': '2024-08-01', 'test_end': '2024-10-31'
        },
        # year 2025 fold
        {
            'train_start': '2024-11-01', 'train_end': '2025-08-31',
            'test_start': '2025-08-01', 'test_end': '2025-10-31'
        }
    ],

    # runtime options
    'max_combinations': 5000,   # if grid size > this, sample randomly
    'random_seed': 42,

    # output
    'results_dir': 'results',
    'data_dir': 'data',
    # plotting
    'plot_top_n': 5,
    # rolling folds generator (optional). If enabled, these params will be used to create
    # folds automatically and override `folds` above. Example: 30-day train window, step 14 days,
    # 14-day test window.
    'rolling': {
        'enabled': False,
        # overall span for generating rolling folds (inclusive)
        'start_date': '2023-01-01',
        'end_date': '2025-12-31',
        'train_window_days': 30,
        'step_days': 14,
        'test_window_days': 14
    },
}

# -----------------
# Helpers
# -----------------

def ensure_dirs():
    os.makedirs(CONFIG['results_dir'], exist_ok=True)
    os.makedirs(CONFIG['data_dir'], exist_ok=True)


def param_product(grid: Dict[str, List[Any]]) -> List[Dict[str, Any]]:
    keys = list(grid.keys())
    vals = [grid[k] for k in keys]
    combos = [dict(zip(keys, comb)) for comb in product(*vals)]
    return combos


def validate_and_fix_folds(folds: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Ensure each fold's train_end is strictly before test_start. If not, adjust train_end to be
    just before test_start and print a warning. Returns modified folds (in-place modified as well).
    """
    fixed = []
    for i, f in enumerate(folds):
        try:
            t_start = pd.to_datetime(f['train_start'])
            t_end = pd.to_datetime(f['train_end'])
            te_start = pd.to_datetime(f['test_start'])
            te_end = pd.to_datetime(f['test_end'])
        except Exception:
            print(f"Warning: invalid date format in fold #{i}: {f}")
            fixed.append(f)
            continue

        if t_end >= te_start:
            # adjust train_end to be immediately before test_start
            new_train_end = te_start - pd.Timedelta(milliseconds=1)
            print(f"Warning: fold #{i} has overlapping train_end ({t_end.date()}) >= test_start ({te_start.date()}).")
            print(f"Adjusting train_end {t_end.date()} -> {new_train_end.date()} to avoid leakage.")
            f['train_end'] = new_train_end.strftime('%Y-%m-%d')
        fixed.append(f)
    return fixed


def generate_rolling_folds(rolling_cfg: Dict[str, Any]) -> List[Dict[str, str]]:
    """Generate rolling folds from a rolling configuration dict.
    rolling_cfg expected keys: start_date, end_date, train_window_days, step_days, test_window_days
    Returns list of folds with string dates (YYYY-MM-DD).
    """
    try:
        start = pd.to_datetime(rolling_cfg['start_date'])
        end = pd.to_datetime(rolling_cfg['end_date'])
        train_len = int(rolling_cfg.get('train_window_days', 30))
        step = int(rolling_cfg.get('step_days', 14))
        test_len = int(rolling_cfg.get('test_window_days', step))
    except Exception:
        raise ValueError('Invalid rolling configuration')

    folds: List[Dict[str, str]] = []
    cur = start
    # generate while there is room for at least a test window
    while True:
        train_start = cur
        train_end = train_start + pd.Timedelta(days=train_len) - pd.Timedelta(days=1)
        test_start = train_end + pd.Timedelta(days=1)
        test_end = test_start + pd.Timedelta(days=test_len) - pd.Timedelta(days=1)
        if test_start > end:
            break
        # if test_end goes beyond overall end, clamp it
        if test_end > end:
            test_end = end
        folds.append({
            'train_start': train_start.strftime('%Y-%m-%d'),
            'train_end': train_end.strftime('%Y-%m-%d'),
            'test_start': test_start.strftime('%Y-%m-%d'),
            'test_end': test_end.strftime('%Y-%m-%d')
        })
        # advance by step days
        cur = cur + pd.Timedelta(days=step)
        # stop if next train_start would be beyond end
        if cur > end:
            break
    return folds


def sample_combinations(all_combos: List[Dict[str, Any]], max_comb: int, seed: int) -> List[Dict[str, Any]]:
    if len(all_combos) <= max_comb:
        return all_combos
    random.Random(seed).shuffle(all_combos)
    return all_combos[:max_comb]


def load_data(coin: str, timeframe: str, start_date: str, end_date: str, futures: bool) -> pd.DataFrame:
    """Fetch or load cached OHLCV for the requested range.
    This function supports storing multiple smaller parquet files per-asset/timeframe
    to avoid massive single-file caches. Cache layout:

    data/{coin}_{timeframe}_{futures}/YYYY-MM-DD_YYYY-MM-DD.parquet

    When asked for a range, existing parquet files that overlap the requested span
    will be concatenated. Missing spans will be downloaded (in 30-day chunks) if
    `BinanceDataCollector` is available; otherwise the function returns whatever
    cached data is present (possibly incomplete) and logs warnings.
    Returns DataFrame with columns: timestamp, open, high, low, close, volume
    """
    # prepare cache directory for this coin/timeframe/futures
    cache_dir = os.path.join(CONFIG['data_dir'], f"{coin}_{timeframe}_{'futures' if futures else 'spot'}")
    os.makedirs(cache_dir, exist_ok=True)

    req_start = pd.to_datetime(start_date)
    req_end = pd.to_datetime(end_date)

    # Helper to parse cache filenames of the form YYYY-MM-DD_YYYY-MM-DD.parquet
    def _parse_range_from_fname(fname: str):
        base = os.path.splitext(fname)[0]
        parts = base.split('_')
        if len(parts) >= 2:
            try:
                s = pd.to_datetime(parts[0])
                e = pd.to_datetime(parts[1])
                return s, e
            except Exception:
                return None
        return None

    # Index file to record known chunk ranges for quick coverage checks
    cache_index_path = os.path.join(cache_dir, 'cache_index.json')

    def _write_index():
        try:
            ranges = []
            for f in os.listdir(cache_dir):
                if not f.endswith('.parquet'):
                    continue
                parsed = _parse_range_from_fname(f)
                if parsed is None:
                    continue
                fs, fe = parsed
                ranges.append([fs.strftime('%Y-%m-%d'), fe.strftime('%Y-%m-%d')])
            if not ranges:
                # remove index if empty
                if os.path.exists(cache_index_path):
                    try:
                        os.remove(cache_index_path)
                    except Exception:
                        pass
                return
            idx = {
                'ranges': ranges,
                'updated': datetime.utcnow().isoformat()
            }
            tmp = cache_index_path + '.tmp'
            with open(tmp, 'w', encoding='utf-8') as fh:
                json.dump(idx, fh, indent=2)
            try:
                if os.path.exists(cache_index_path):
                    os.remove(cache_index_path)
            except Exception:
                pass
            os.replace(tmp, cache_index_path)
        except Exception:
            # non-fatal
            pass

    def _read_index():
        try:
            if not os.path.exists(cache_index_path):
                return None
            with open(cache_index_path, 'r', encoding='utf-8') as fh:
                return json.load(fh)
        except Exception:
            return None

    available_files = []
    for f in os.listdir(cache_dir):
        if not f.endswith('.parquet'):
            continue
        parsed = _parse_range_from_fname(f)
        if parsed is None:
            # skip files that don't follow the simple naming convention
            continue
        s, e = parsed
        available_files.append((s, e, os.path.join(cache_dir, f)))

    # Quick coverage check using filename ranges (and cache_index if present).
    # Build merged intervals from available file name ranges.
    def _merge_intervals(ranges: List[Tuple[pd.Timestamp, pd.Timestamp]]):
        if not ranges:
            return []
        # sort by start
        ranges_sorted = sorted(ranges, key=lambda x: x[0])
        merged = [list(ranges_sorted[0])]
        for a, b in ranges_sorted[1:]:
            last_s, last_e = pd.to_datetime(merged[-1][0]), pd.to_datetime(merged[-1][1])
            if a <= last_e + pd.Timedelta(days=0):
                # overlap or contiguous: extend
                merged[-1][1] = max(last_e, b).strftime('%Y-%m-%d')
            else:
                merged.append([a.strftime('%Y-%m-%d'), b.strftime('%Y-%m-%d')])
        # convert back to timestamps
        return [(pd.to_datetime(s), pd.to_datetime(e)) for s, e in merged]

    file_ranges = [(s, e) for s, e, _ in available_files]
    merged = _merge_intervals(file_ranges)

    # If merged intervals fully cover requested window, we won't need to download.
    def _is_fully_covered(merged_intervals, req_s, req_e):
        if not merged_intervals:
            return False
        for ms, me in merged_intervals:
            if ms <= req_s and me >= req_e:
                return True
        return False

    if _is_fully_covered(merged, req_start, req_end):
        # we already collected overlapping files above and will read them below
        need_download = False

    # Also support legacy single-file cache placed at data/ root (legacy behavior)
    legacy_fname = os.path.join(CONFIG['data_dir'], f"{coin}_{timeframe}_{start_date}_{end_date}_{'futures' if futures else 'spot'}.parquet")
    if os.path.exists(legacy_fname):
        try:
            df = pd.read_parquet(legacy_fname)
            return df
        except Exception:
            # fall through to directory-based approach
            pass

    # Collect dataframes from available files that overlap the requested range
    parts = []
    for s, e, path in available_files:
        # overlap check
        if not (e < req_start or s > req_end):
            try:
                dfi = pd.read_parquet(path)
                parts.append(dfi)
            except Exception as ex:
                print('Warning: failed to read cached segment', path, ex)

    if parts:
        df_full = pd.concat(parts, ignore_index=True).sort_values('timestamp').reset_index(drop=True)
        # trim to requested window
        mask = (pd.to_datetime(df_full['timestamp'], unit='ms') >= req_start) & (pd.to_datetime(df_full['timestamp'], unit='ms') <= req_end)
        df_trim = df_full.loc[mask].reset_index(drop=True)
    else:
        df_trim = pd.DataFrame()

    # Determine if coverage is complete
    def _ts_ms(dt: pd.Timestamp) -> int:
        return int(pd.to_datetime(dt).timestamp() * 1000)

    need_download = False
    if df_trim.empty:
        need_download = True
    else:
        mn = pd.to_datetime(df_trim['timestamp'].min(), unit='ms')
        mx = pd.to_datetime(df_trim['timestamp'].max(), unit='ms')
        if mn > req_start or mx < req_end:
            need_download = True

    if need_download:
        if not _HAS_BINANCE:
            print(f"Warning: cached data incomplete for {coin} {timeframe} {start_date}->{end_date} and no downloader available.")
            return df_trim

        # Download missing spans in chunks to avoid single large downloads and save each chunk
        collector = BinanceDataCollector(futures=futures)
        # download/resilience configuration (override via CONFIG['download'])
        dl_cfg = CONFIG.get('download', {})
        CHUNK_DAYS = int(dl_cfg.get('chunk_days', 30))
        RETRIES = int(dl_cfg.get('retries', 5))
        BACKOFF = float(dl_cfg.get('backoff_factor', 2.0))
        SLEEP_BETWEEN = float(dl_cfg.get('sleep_between_chunks', 0.5))
        JITTER = float(dl_cfg.get('jitter', 0.2))

        spans = []
        cur = req_start
        while cur <= req_end:
            chunk_end = min(req_end, cur + pd.Timedelta(days=CHUNK_DAYS - 1))
            spans.append((cur, chunk_end))
            cur = chunk_end + pd.Timedelta(days=1)

        downloaded_parts = []
        for s, e in spans:
            fname = f"{s.strftime('%Y-%m-%d')}_{e.strftime('%Y-%m-%d')}.parquet"
            out_path = os.path.join(cache_dir, fname)
            # If any existing chunk file fully covers the requested span, reuse it.
            # This handles the case where previously-saved chunks were named by actual
            # returned coverage (so `out_path` may not exist under the requested name).
            reused = False
            try:
                for f in os.listdir(cache_dir):
                    if not f.endswith('.parquet'):
                        continue
                    parsed = _parse_range_from_fname(f)
                    if parsed is None:
                        continue
                    fs, fe = parsed
                    # if this chunk fully covers requested [s,e], reuse it
                    if fs <= s and fe >= e:
                        try:
                            existing_path = os.path.join(cache_dir, f)
                            dfi = pd.read_parquet(existing_path)
                            downloaded_parts.append(dfi)
                            reused = True
                            break
                        except Exception:
                            # if reading fails, ignore and allow re-download
                            pass
            except Exception:
                # if listing fails, fall back to previous existence check below
                reused = False

            if reused:
                continue

            # If file with the requested name already exists (unlikely if saved by actual range), load it
            if os.path.exists(out_path):
                try:
                    dfi = pd.read_parquet(out_path)
                    downloaded_parts.append(dfi)
                    continue
                except Exception:
                    pass

            # attempt with retries + exponential backoff + jitter
            attempt = 0
            success = False
            last_exc = None
            while attempt <= RETRIES and not success:
                try:
                    if attempt > 0:
                        # backoff delay before retrying
                        delay = SLEEP_BETWEEN * (BACKOFF ** (attempt - 1))
                        # apply jitter
                        delay = delay * (1.0 + random.uniform(-JITTER, JITTER))
                        delay = max(0.0, delay)
                        print(f"Retrying chunk {coin} {timeframe} {s.date()}->{e.date()} attempt={attempt}/{RETRIES} after {delay:.2f}s")
                        time.sleep(delay)

                    print(f"Downloading chunk {coin} {timeframe} {s.date()}->{e.date()} (attempt {attempt+1}/{RETRIES+1}) ...")
                    dfi = collector.fetch_candles(coin, timeframe, start_date=s.strftime('%Y-%m-%d'), end_date=e.strftime('%Y-%m-%d'))
                    # consider empty or malformed returns as retryable failures
                    if isinstance(dfi, pd.DataFrame) and not dfi.empty and 'timestamp' in dfi.columns:
                        # determine actual coverage from returned data and name file accordingly
                        try:
                            ts_min = int(dfi['timestamp'].min())
                            ts_max = int(dfi['timestamp'].max())
                            # infer ms vs s
                            if ts_min < 1e11:
                                # timestamps look like seconds; convert to ms for consistency
                                ts_min_ms = ts_min * 1000
                                ts_max_ms = ts_max * 1000
                            else:
                                ts_min_ms = ts_min
                                ts_max_ms = ts_max
                            start_date_actual = pd.to_datetime(ts_min_ms, unit='ms').strftime('%Y-%m-%d')
                            end_date_actual = pd.to_datetime(ts_max_ms, unit='ms').strftime('%Y-%m-%d')
                            actual_fname = f"{start_date_actual}_{end_date_actual}.parquet"
                            actual_path = os.path.join(cache_dir, actual_fname)
                            tmp_actual = actual_path + ".tmp"
                            try:
                                dfi.to_parquet(tmp_actual, index=False)
                                # atomic replace
                                try:
                                    if os.path.exists(actual_path):
                                        os.remove(actual_path)
                                except Exception:
                                    pass
                                os.replace(tmp_actual, actual_path)
                                print(f"Saved chunk for {coin} {timeframe} actual {start_date_actual}->{end_date_actual} -> {actual_path}")
                                # update cache index after successful save
                                try:
                                    _write_index()
                                except Exception:
                                    pass
                            finally:
                                if os.path.exists(tmp_actual):
                                    try:
                                        os.remove(tmp_actual)
                                    except Exception:
                                        pass
                        except Exception as write_e:
                            print('Warning: failed to write chunk cache', out_path, write_e)
                            # fallback: try to write to requested out_path
                            try:
                                dfi.to_parquet(out_path, index=False)
                            except Exception:
                                print('Warning: also failed to write fallback chunk cache', out_path)
                        downloaded_parts.append(dfi)
                        success = True
                        break
                    else:
                        last_exc = None
                        print('Warning: downloader returned no data for chunk', s, e)
                        # treat as retryable; increment attempt and loop
                except Exception as dl_e:
                    last_exc = dl_e
                    print('Warning: failed to download chunk', s, e, dl_e)
                attempt += 1

            if not success:
                if last_exc is not None:
                    print(f"Failed to download chunk {s.date()}->{e.date()} after {RETRIES} retries: {last_exc}")
                else:
                    print(f"Failed to download chunk {s.date()}->{e.date()} after {RETRIES} retries: no data returned")

        # Combine cached parts and newly downloaded parts
        all_parts = []
        if not df_trim.empty:
            all_parts.append(df_trim)
        if downloaded_parts:
            all_parts.extend(downloaded_parts)

        if not all_parts:
            return pd.DataFrame()

        df_full = pd.concat(all_parts, ignore_index=True).sort_values('timestamp').reset_index(drop=True)
        # final trim
        mask = (pd.to_datetime(df_full['timestamp'], unit='ms') >= req_start) & (pd.to_datetime(df_full['timestamp'], unit='ms') <= req_end)
        df_final = df_full.loc[mask].reset_index(drop=True)
        return df_final

    # if no download needed, return trimmed df (could be empty)
    return df_trim


def _timeframe_to_seconds(timeframe: str) -> int:
    """Convert timeframe like '1m','15m','1h' to seconds."""
    tf = timeframe.lower().strip()
    if tf.endswith('m'):
        return int(tf[:-1]) * 60
    if tf.endswith('h'):
        return int(tf[:-1]) * 3600
    if tf.endswith('d'):
        return int(tf[:-1]) * 86400
    # fallback assume minutes
    try:
        return int(tf) * 60
    except Exception:
        return 60


def _validate_and_fix_timeframe(df: pd.DataFrame, requested_timeframe: str, cache_path: str = None) -> pd.DataFrame:
    """Check median sample delta of df and warn if it doesn't match requested_timeframe.
    Returns df as-is without resampling. User should download data at the correct timeframe.
    """
    if 'timestamp' not in df.columns:
        return df
    if len(df) < 3:
        return df
    # compute median delta in seconds
    deltas = pd.to_datetime(df['timestamp'], unit='ms').diff().dt.total_seconds().dropna()
    if deltas.empty:
        return df
    median_delta = float(deltas.median())
    expected = _timeframe_to_seconds(requested_timeframe)
    
    # Warn if data doesn't match requested timeframe
    if median_delta < expected * 0.9:
        print(f"Warning: data resolution ({median_delta}s) is finer than requested timeframe {requested_timeframe} ({expected}s). Please download {requested_timeframe} data directly.")
    elif median_delta > expected * 1.1:
        print(f"Warning: data resolution ({median_delta}s) is coarser than requested timeframe {requested_timeframe} ({expected}s). Please download {requested_timeframe} data directly.")
    
    return df


def compute_trade_metrics(positions: List[Any], initial_capital: float, equity_df: pd.DataFrame = None) -> Dict[str, Any]:
    """Compute metrics from list of Position dataclasses returned by detect_order_blocks.
    If `equity_df` is provided (DataFrame with columns ['time','equity']), compute Sharpe from
    the equity time-series (preferred). Otherwise fallback to per-trade Sharpe as before.
    """
    # Filter closed positions
    closed = [p for p in positions if getattr(p, 'pnl_dollars', None) is not None]
    num_trades = len(closed)
    if num_trades == 0:
        return {
            'num_trades': 0,
            'total_pnl': 0.0,
            'roi_pct': 0.0,
            'win_rate': 0.0,
            'avg_win': 0.0,
            'avg_loss': 0.0,
            'profit_factor': 0.0,
            'sharpe': 0.0,
            'max_drawdown': 0.0
        }

    wins = [p for p in closed if p.pnl_dollars > 0]
    losses = [p for p in closed if p.pnl_dollars < 0]

    total_pnl = sum(p.pnl_dollars for p in closed)
    gross_win = sum(p.pnl_dollars for p in wins) if wins else 0.0
    gross_loss = -sum(p.pnl_dollars for p in losses) if losses else 0.0
    profit_factor = (gross_win / gross_loss) if gross_loss > 0 else (float('inf') if gross_win>0 else 0.0)
    win_rate = len(wins) / num_trades if num_trades>0 else 0.0
    avg_win = (sum(p.pnl_dollars for p in wins) / len(wins)) if wins else 0.0
    avg_loss = (sum(p.pnl_dollars for p in losses) / len(losses)) if losses else 0.0

    max_dd = 0.0
    sharpe = 0.0

    # If an equity time-series was provided, compute Sharpe and drawdown from it
    if equity_df is not None and not equity_df.empty and 'equity' in equity_df.columns:
        edf = equity_df.copy()
        # ensure sorted by time
        edf = edf.sort_values('time').reset_index(drop=True)
        # compute drawdown
        roll_max = edf['equity'].cummax()
        drawdown = (edf['equity'] - roll_max)
        max_dd = drawdown.min() if not drawdown.empty else 0.0

        # compute periodic returns from equity series
        # convert time column (ms) to datetime if needed
        try:
            times = pd.to_datetime(edf['time'], unit='ms')
        except Exception:
            times = pd.to_datetime(edf['time'])
        returns = edf['equity'].pct_change().dropna().astype(float)
        if len(returns) > 1 and returns.std() > 0:
            # estimate periods per year from median delta
            deltas = times.diff().dt.total_seconds().dropna()
            median_delta = deltas.median() if not deltas.empty else 24*3600
            periods_per_year = (365.0 * 24 * 3600) / median_delta
            sharpe = (returns.mean() / returns.std()) * math.sqrt(periods_per_year)
        else:
            sharpe = 0.0
    else:
        # fallback: per-trade Sharpe using closed trades
        trade_returns = np.array([p.pnl_dollars / initial_capital for p in closed])
        if trade_returns.size <= 1 or np.std(trade_returns) == 0:
            sharpe = 0.0
        else:
            sharpe = (np.mean(trade_returns) / np.std(trade_returns)) * math.sqrt(len(trade_returns))

    roi_pct = (total_pnl / initial_capital) * 100.0

    return {
        'num_trades': num_trades,
        'total_pnl': total_pnl,
        'roi_pct': roi_pct,
        'win_rate': win_rate,
        'avg_win': avg_win,
        'avg_loss': avg_loss,
        'profit_factor': profit_factor,
        'sharpe': sharpe,
        'max_drawdown': max_dd
    }


def evaluate_combo_on_fold(combo: Dict[str,Any], coin: str, timeframe: str, futures: bool, fold: Dict[str,str]) -> Dict[str,Any]:
    """Run detect_order_blocks on train and test for a given fold and return metrics."""
    # Load train and test data
    train_df = load_data(coin, timeframe, fold['train_start'], fold['train_end'], futures)
    test_df = load_data(coin, timeframe, fold['test_start'], fold['test_end'], futures)

    # derive OB helpers from combo (allow specifying fraction of swing_length)
    ob_search_window_frac = float(combo.get('ob_search_window_frac', 0.5))
    ob_search_window = max(1, int(round(ob_search_window_frac * combo.get('swing_length', 1))))
    break_atr_mult = float(combo.get('break_atr_mult', CONFIG['fixed'].get('break_atr_mult', 0.2)))
    ob_min_sl_atr_mult = float(combo.get('ob_min_sl_atr_mult', CONFIG['fixed'].get('ob_min_sl_atr_mult', 0.5)))
    ob_min_sl_pct = float(combo.get('ob_min_sl_pct', CONFIG['fixed'].get('ob_min_sl_pct', 0.001)))

    params = {
        # strategy parameters mapped from combo
        'swing_length': combo['swing_length'],
        'hide_overlap': True,
        'show_last_x_ob': 10,
        'ob_search_window': ob_search_window,
        'violation_type': combo['violation_type'],
        'min_strength_ratio': combo['min_strength_ratio'],
        'stop_loss_multiplier': combo['stop_loss_multiplier'],
        'take_profit_multiplier': combo['take_profit_multiplier'],
        'break_atr_mult': break_atr_mult,
        'ob_min_sl_atr_mult': ob_min_sl_atr_mult,
        'ob_min_sl_pct': ob_min_sl_pct,
        'max_concurrent_positions': CONFIG['fixed']['max_concurrent_positions'],
        'initial_capital': CONFIG['fixed']['initial_capital'],
        'risk_per_trade_percent': combo['risk_per_trade_percent'],
        'trailing_stop_activation': combo['trailing_stop_activation'],
        'trailing_stop_percent': combo['trailing_stop_percent'],
        'trailing_stop_buffer_candles': CONFIG['fixed']['trailing_stop_buffer_candles'],
        'trailing_stop_update_threshold': CONFIG['fixed']['trailing_stop_update_threshold'],
        'use_fixed_capital': CONFIG['fixed']['use_fixed_capital'],
        'max_position_size_usd': 100000.0,
        'entry_price_mode': CONFIG['fixed']['entry_price_mode'],
        'commission_percent': CONFIG['fixed']['commission_percent'],
        'slippage_percent': CONFIG['fixed']['slippage_percent']
    }

    # Run on train
    try:
        obs_t, sig_t, pos_t, eq_t = detect_order_blocks(train_df, **params)
    except Exception as e:
        print('Error running train backtest for', combo, e)
        return {'error': str(e)}

    train_metrics = compute_trade_metrics(pos_t, params['initial_capital'], equity_df=eq_t)

    # Run on test
    try:
        obs_te, sig_te, pos_te, eq_te = detect_order_blocks(test_df, **params)
    except Exception as e:
        print('Error running test backtest for', combo, e)
        return {'error': str(e)}

    test_metrics = compute_trade_metrics(pos_te, params['initial_capital'], equity_df=eq_te)

    out = {
        'combo': combo,
        'coin': coin,
        'timeframe': timeframe,
        'futures': futures,
        'train_metrics': train_metrics,
        'test_metrics': test_metrics
    }
    return out


def evaluate_combo_with_full_df(combo: Dict[str,Any], coin: str, timeframe: str, futures: bool, full_df: pd.DataFrame, folds: List[Dict[str,str]]) -> Dict[str,Any]:
    """Evaluate a combo using a preloaded full DataFrame (covering all folds). Returns aggregated result dict similar to evaluate_combo_on_fold outputs aggregated across folds."""
    per_fold = []
    for fold in folds:
        # slice train and test from full_df by timestamp (assume timestamp in ms)
        # convert fold dates to ms since epoch
        train_start_ms = int(pd.to_datetime(fold['train_start']).timestamp() * 1000)
        train_end_ms = int(pd.to_datetime(fold['train_end']).timestamp() * 1000)
        test_start_ms = int(pd.to_datetime(fold['test_start']).timestamp() * 1000)
        test_end_ms = int(pd.to_datetime(fold['test_end']).timestamp() * 1000)

        train_df = full_df[(full_df['timestamp'] >= train_start_ms) & (full_df['timestamp'] <= train_end_ms)].reset_index(drop=True)
        test_df = full_df[(full_df['timestamp'] >= test_start_ms) & (full_df['timestamp'] <= test_end_ms)].reset_index(drop=True)

        if train_df.empty or test_df.empty:
            return {'error': f'empty_data_for_fold {fold}'}

        # derive OB helpers from combo (allow specifying fraction of swing_length)
        ob_search_window_frac = float(combo.get('ob_search_window_frac', 0.5))
        ob_search_window = max(1, int(round(ob_search_window_frac * combo.get('swing_length', 1))))
        break_atr_mult = float(combo.get('break_atr_mult', CONFIG['fixed'].get('break_atr_mult', 0.2)))
        ob_min_sl_atr_mult = float(combo.get('ob_min_sl_atr_mult', CONFIG['fixed'].get('ob_min_sl_atr_mult', 0.5)))
        ob_min_sl_pct = float(combo.get('ob_min_sl_pct', CONFIG['fixed'].get('ob_min_sl_pct', 0.001)))

        params = {
            'swing_length': combo['swing_length'],
            'hide_overlap': True,
            'show_last_x_ob': 10,
            'ob_search_window': ob_search_window,
            'violation_type': combo['violation_type'],
            'min_strength_ratio': combo['min_strength_ratio'],
            'stop_loss_multiplier': combo['stop_loss_multiplier'],
            'take_profit_multiplier': combo['take_profit_multiplier'],
            'break_atr_mult': break_atr_mult,
            'ob_min_sl_atr_mult': ob_min_sl_atr_mult,
            'ob_min_sl_pct': ob_min_sl_pct,
            'max_concurrent_positions': CONFIG['fixed']['max_concurrent_positions'],
            'initial_capital': CONFIG['fixed']['initial_capital'],
            'risk_per_trade_percent': combo['risk_per_trade_percent'],
            'trailing_stop_activation': combo['trailing_stop_activation'],
            'trailing_stop_percent': combo['trailing_stop_percent'],
            'trailing_stop_buffer_candles': CONFIG['fixed']['trailing_stop_buffer_candles'],
            'trailing_stop_update_threshold': CONFIG['fixed']['trailing_stop_update_threshold'],
            'use_fixed_capital': CONFIG['fixed']['use_fixed_capital'],
            'max_position_size_usd': 100000.0,
            'entry_price_mode': CONFIG['fixed']['entry_price_mode'],
            'commission_percent': CONFIG['fixed']['commission_percent'],
            'slippage_percent': CONFIG['fixed']['slippage_percent']
        }

        try:
            obs_t, sig_t, pos_t, eq_t = detect_order_blocks(train_df, **params)
        except Exception as e:
            return {'error': f'train_error: {e}'}
        train_metrics = compute_trade_metrics(pos_t, params['initial_capital'], equity_df=eq_t)

        try:
            obs_te, sig_te, pos_te, eq_te = detect_order_blocks(test_df, **params)
        except Exception as e:
            return {'error': f'test_error: {e}'}
        test_metrics = compute_trade_metrics(pos_te, params['initial_capital'], equity_df=eq_te)

        # build buy-and-hold equity series for the test period to compare
        bh_equity = None
        try:
            if test_df is not None and len(test_df) > 0:
                # entry: first close in test_df
                entry_price = float(test_df['close'].iat[0])
                initial_cap = params['initial_capital']
                # guard against invalid entry price (zero/NaN)
                if entry_price is None or entry_price != entry_price or entry_price <= 0:
                    # can't build buy-and-hold series if entry price is invalid
                    bh_equity = None
                else:
                    # position size (units) ignoring fractional constraints
                    position_size = initial_cap / entry_price if entry_price > 0 else 0.0
                # entry fees per unit (commission+slippage percent)
                    entry_fee_per_unit = entry_price * ((params['commission_percent'] + params['slippage_percent']) / 100.0)
                    entry_fees_total = entry_fee_per_unit * position_size
                    # per-bar equity: initial_capital minus entry fees + unrealized PnL
                    times = test_df['timestamp'].tolist()
                    equities = []
                    for i in range(len(test_df)):
                        mark = float(test_df['close'].iat[i])
                        unreal = (mark - entry_price) * position_size
                        eq = initial_cap - entry_fees_total + unreal
                        equities.append({'time': int(test_df['timestamp'].iat[i]), 'equity': float(eq)})
                    bh_equity = pd.DataFrame(equities)
        except Exception:
            bh_equity = None

        per_fold.append({'train_metrics': train_metrics, 'test_metrics': test_metrics, 'eq_test': eq_te if (eq_te is not None) else pd.DataFrame(), 'eq_bh': bh_equity if (bh_equity is not None) else pd.DataFrame()})

    # aggregate
    test_sharpes = [f['test_metrics']['sharpe'] for f in per_fold]
    mean_test_sharpe = float(np.mean(test_sharpes)) if test_sharpes else 0.0
    mean_test_roi = float(np.mean([f['test_metrics']['roi_pct'] for f in per_fold]))
    mean_test_pf = float(np.mean([f['test_metrics']['profit_factor'] for f in per_fold]))
    mean_test_win = float(np.mean([f['test_metrics']['win_rate'] for f in per_fold]))
    mean_test_trades = int(np.mean([f['test_metrics']['num_trades'] for f in per_fold]))

    # pooled test equity: concatenate per-fold test equities (chronological) and compute Sharpe/ROI
    pooled_test_equity = None
    pooled_bh_equity = None
    try:
        eq_dfs = [f['eq_test'] for f in per_fold if isinstance(f.get('eq_test'), pd.DataFrame) and not f.get('eq_test').empty]
        if eq_dfs:
            pooled_test_equity = pd.concat(eq_dfs, ignore_index=True).sort_values('time').reset_index(drop=True)
        bh_eq_dfs = [f['eq_bh'] for f in per_fold if isinstance(f.get('eq_bh'), pd.DataFrame) and not f.get('eq_bh').empty]
        if bh_eq_dfs:
            pooled_bh_equity = pd.concat(bh_eq_dfs, ignore_index=True).sort_values('time').reset_index(drop=True)
    except Exception:
        pooled_test_equity = None
        pooled_bh_equity = None

    def _equity_sharpe_and_roi(equity_df: pd.DataFrame):
        if equity_df is None or equity_df.empty:
            return 0.0, 0.0
        edf = equity_df.sort_values('time').reset_index(drop=True)
        try:
            times = pd.to_datetime(edf['time'], unit='ms')
        except Exception:
            times = pd.to_datetime(edf['time'])
        returns = edf['equity'].pct_change().dropna().astype(float)
        if len(returns) > 1 and returns.std() > 0:
            deltas = times.diff().dt.total_seconds().dropna()
            median_delta = deltas.median() if not deltas.empty else 24*3600
            periods_per_year = (365.0 * 24 * 3600) / median_delta
            sharpe = (returns.mean() / returns.std()) * math.sqrt(periods_per_year)
        else:
            sharpe = 0.0
        # roi: (last equity - first equity) / first_equity *100
        try:
            first_eq = float(edf['equity'].iat[0])
            if first_eq == 0.0:
                roi = 0.0
            else:
                roi = ((edf['equity'].iat[-1] - first_eq) / first_eq) * 100.0
        except Exception:
            roi = 0.0
        return float(sharpe), float(roi)

    pooled_test_sharpe = 0.0
    pooled_test_roi = 0.0
    pooled_bh_sharpe = 0.0
    pooled_bh_roi = 0.0
    if pooled_test_equity is not None and not pooled_test_equity.empty:
        pooled_test_sharpe, pooled_test_roi = _equity_sharpe_and_roi(pooled_test_equity)
    if pooled_bh_equity is not None and not pooled_bh_equity.empty:
        pooled_bh_sharpe, pooled_bh_roi = _equity_sharpe_and_roi(pooled_bh_equity)

    out = {
        'coin': coin,
        'timeframe': timeframe,
        'futures': futures,
        'combo': combo,
        'mean_test_sharpe': mean_test_sharpe,
        'mean_test_roi': mean_test_roi,
        'pooled_test_sharpe': pooled_test_sharpe,
        'pooled_test_roi': pooled_test_roi,
        'pooled_bh_sharpe': pooled_bh_sharpe,
        'pooled_bh_roi': pooled_bh_roi,
        'mean_test_profit_factor': mean_test_pf,
        'mean_test_win_rate': mean_test_win,
        'mean_test_trades': mean_test_trades,
        'per_fold': per_fold,
        'pooled_test_equity': pooled_test_equity,
        'pooled_bh_equity': pooled_bh_equity
    }
    return out


def evaluate_task_worker(task: Tuple[str,str,bool,Dict[str,Any],str,str,List[Dict[str,str]]]) -> Dict[str,Any]:
    """Top-level worker for ProcessPoolExecutor: loads (or downloads) chunked cache via load_data
    and evaluates the combo. The task tuple includes full_start/full_end and folds so workers
    can independently load needed data.
    """
    coin, timeframe, futures, combo, full_start, full_end, folds = task
    try:
        full_df = load_data(coin, timeframe, full_start, full_end, futures)
    except Exception as e:
        return {'coin': coin, 'timeframe': timeframe, 'futures': futures, 'combo': combo, 'error': f'load_data_error: {e}'}
    if full_df is None or (isinstance(full_df, pd.DataFrame) and full_df.empty):
        return {'coin': coin, 'timeframe': timeframe, 'futures': futures, 'combo': combo, 'error': 'empty_or_missing_data'}
    try:
        res = evaluate_combo_with_full_df(combo, coin, timeframe, futures, full_df, folds)
    except Exception as e:
        return {'coin': coin, 'timeframe': timeframe, 'futures': futures, 'combo': combo, 'error': str(e)}

    # Ensure result is a dict and annotate with task identifiers so errors aren't anonymous when
    # evaluate_combo_with_full_df returns only an {'error': ...} dict.
    if not isinstance(res, dict):
        return {'coin': coin, 'timeframe': timeframe, 'futures': futures, 'combo': combo, 'error': f'non_dict_result: {type(res)}'}

    # Fill missing identifying keys
    if 'coin' not in res or res.get('coin') is None:
        res['coin'] = coin
    if 'timeframe' not in res or res.get('timeframe') is None:
        res['timeframe'] = timeframe
    if 'futures' not in res or res.get('futures') is None:
        res['futures'] = futures
    if 'combo' not in res or res.get('combo') is None:
        res['combo'] = combo

    return res


# -----------------
# Main grid search
# -----------------

def run_grid_search(max_combinations: int = None):
    ensure_dirs()
    # Optionally generate rolling folds (overrides explicit CONFIG['folds'] if enabled)
    rolling_cfg = CONFIG.get('rolling', {})
    if rolling_cfg and rolling_cfg.get('enabled'):
        try:
            generated = generate_rolling_folds(rolling_cfg)
            print(f"Generated {len(generated)} rolling folds from {rolling_cfg.get('start_date')} to {rolling_cfg.get('end_date')}")
            CONFIG['folds'] = generated
        except Exception as e:
            print('Warning: failed to generate rolling folds:', e)
    # validate folds to prevent accidental data leakage (auto-fix overlapping train/test)
    CONFIG['folds'] = validate_and_fix_folds(CONFIG.get('folds', []))
    grid = CONFIG['grid']
    all_combos = param_product(grid)
    total = len(all_combos)
    max_comb = max_combinations or CONFIG['max_combinations']
    print(f"Total grid combinations: {total}")
    if total > max_comb:
        print(f"Sampling {max_comb} combinations out of {total} (seed={CONFIG['random_seed']})")
        combos = sample_combinations(all_combos, max_comb, CONFIG['random_seed'])
    else:
        combos = all_combos

    print(f"Running combos: {len(combos)}")

    results = []
    # Preload full-range data to disk (one file per coin/timeframe/futures) to avoid repeated downloads
    data_files: Dict[Tuple[str,str,bool], str] = {}
    for coin in CONFIG['coins']:
        for timeframe in CONFIG['timeframes']:
            for futures in CONFIG['futures_options']:
                # determine full range covering all folds
                train_starts = [f['train_start'] for f in CONFIG['folds']]
                test_ends = [f['test_end'] for f in CONFIG['folds']]
                full_start = min(train_starts)
                full_end = max(test_ends)
                # construct expected cache filename/path (may or may not exist)
                filename = f"{coin}_{timeframe}_{full_start}_{full_end}_{'futures' if futures else 'spot'}.parquet"
                path = os.path.join(CONFIG['data_dir'], filename)
                # load_data will cache to parquet if not present. If a cache exists but
                # doesn't cover the requested timeframe, attempt to re-download (if collector available).
                try:
                    df_full = load_data(coin, timeframe, full_start, full_end, futures)
                except Exception as e:
                    print(f"Failed to load data for {coin} {timeframe} futures={futures}: {e}")
                    continue

                # For backward compatibility with the rest of the code which expects a
                # single-parquet file path for each (coin,timeframe,futures) key, persist
                # the concatenated DataFrame returned by load_data to the legacy single
                # filename (path). This keeps evaluate_task_worker simple (it reads one
                # parquet path) while still using chunked caches behind the scenes.
                try:
                    if isinstance(df_full, pd.DataFrame) and not df_full.empty:
                        # write atomically: write to temp then rename
                        tmp_path = path + ".tmp"
                        try:
                            df_full.to_parquet(tmp_path, index=False)
                            try:
                                if os.path.exists(path):
                                    os.remove(path)
                            except Exception:
                                pass
                            os.replace(tmp_path, path)
                        finally:
                            if os.path.exists(tmp_path):
                                try:
                                    os.remove(tmp_path)
                                except Exception:
                                    pass
                except Exception as e:
                    print('Warning: failed to persist combined cache to legacy path', path, e)

                # Quick coverage sanity check: if timestamps don't cover the requested span,
                # try to refresh the cache by removing the stale file and re-downloading (if possible).
                try:
                    if isinstance(df_full, pd.DataFrame) and 'timestamp' in df_full.columns:
                        mn = pd.to_datetime(df_full['timestamp'].min(), unit='ms')
                        mx = pd.to_datetime(df_full['timestamp'].max(), unit='ms')
                        req_start = pd.to_datetime(full_start)
                        req_end = pd.to_datetime(full_end)
                        if mn > req_start or mx < req_end:
                            print(f"Warning: cached data {path} coverage {mn}..{mx} does not cover requested {req_start}..{req_end}")
                            if _HAS_BINANCE:
                                print("Attempting to re-download full-range data (overwriting cache)...")
                                try:
                                    if os.path.exists(path):
                                        os.remove(path)
                                except Exception as rm_e:
                                    print('Failed to remove stale cache file:', rm_e)
                                try:
                                    df_full = load_data(coin, timeframe, full_start, full_end, futures)
                                except Exception as dl_e:
                                    print('Re-download failed for', coin, timeframe, futures, dl_e)
                                    # skip this key if re-download fails
                                    continue
                            else:
                                print('No downloader available to refresh cache; skipping this key.')
                                continue
                except Exception:
                    # non-fatal: proceed with what we have
                    pass

                # Register this asset/timeframe by storing the full requested start/end dates
                # as the worker will call `load_data(...)` directly. If df_full is empty we
                # skip this key to avoid launching workers that will have no data.
                if isinstance(df_full, pd.DataFrame) and not df_full.empty:
                    data_files[(coin, timeframe, futures)] = (full_start, full_end)
                else:
                    chunk_dir = os.path.join(CONFIG['data_dir'], f"{coin}_{timeframe}_{'futures' if futures else 'spot'}")
                    print(f"Warning: no data available for {coin} {timeframe} {full_start}->{full_end}; skipping this asset/timeframe.\n"
                          f"Chunked cache may be available under {chunk_dir} -- consider re-running with smaller chunks or enabling downloader settings.")

    # Build task list: each task is (coin, timeframe, futures, combo, full_start, full_end, folds)
    tasks = []
    for coin in CONFIG['coins']:
        for timeframe in CONFIG['timeframes']:
            for futures in CONFIG['futures_options']:
                key = (coin, timeframe, futures)
                if key not in data_files:
                    continue
                full_start, full_end = data_files[key]
                for combo in combos:
                    # include folds explicitly so worker processes use validated folds
                    tasks.append((coin, timeframe, futures, combo, full_start, full_end, CONFIG['folds']))

    print(f"Total tasks: {len(tasks)} (combos x assets)")
    start_time = time.time()

    # use top-level evaluate_task_worker when running in parallel

    # run tasks in parallel using processes (detect_order_blocks is CPU-bound)
    max_workers = max(1, (os.cpu_count() or 1) - 1)
    results = []
    if tqdm is not None:
        pbar = tqdm(total=len(tasks), desc='Grid tasks')
    else:
        pbar = None

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as exe:
        futures_map = {exe.submit(evaluate_task_worker, task): task for task in tasks}
        for fut in concurrent.futures.as_completed(futures_map):
            res = fut.result()
            results.append(res)
            if pbar is not None:
                pbar.update(1)
            else:
                if len(results) % 50 == 0:
                    elapsed = time.time() - start_time
                    print(f"Processed {len(results)}/{len(tasks)} tasks - elapsed {elapsed:.1f}s")
    if pbar is not None:
        pbar.close()

    # Diagnose task errors: save any task-level errors to a JSON file and print a short summary.
    try:
        errors = [r for r in results if r.get('error')]
        num_with_combo = sum(1 for r in results if r.get('combo') is not None)
        num_success = sum(1 for r in results if r.get('combo') is not None and not r.get('error'))
        if errors:
            err_ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            err_path = os.path.join(CONFIG['results_dir'], f'grid_search_errors_{err_ts}.json')
            try:
                with open(err_path, 'w', encoding='utf-8') as ef:
                    json.dump(errors, ef, default=str, indent=2)
                print(f"Saved {len(errors)} task errors to {err_path}")
            except Exception as e:
                print('Warning: failed to write errors file', e)
        print(f"Tasks completed: {len(results)} total, {num_with_combo} returned combo, {num_success} successful (no error), {len(errors)} errors")
        if errors:
            print('Sample errors (up to 5):')
            for e in errors[:5]:
                print('-', e.get('error'), '->', {k: e.get(k) for k in ('coin','timeframe','futures','combo')})
    except Exception:
        pass

    # Save results
    ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
    out_path = os.path.join(CONFIG['results_dir'], f'grid_search_results_{ts}.parquet')
    # Flatten results for CSV-friendly layout
    rows = []
    for r in results:
        base = {
            'coin': r.get('coin'),
            'timeframe': r.get('timeframe'),
            'futures': r.get('futures')
        }
        combo = r.get('combo')
        if combo is None:
            continue
        flat = {**base, **combo}
        flat['mean_test_sharpe'] = r.get('mean_test_sharpe')
        # pooled metrics (if present)
        flat['pooled_test_sharpe'] = r.get('pooled_test_sharpe')
        flat['pooled_test_roi'] = r.get('pooled_test_roi')
        flat['pooled_bh_sharpe'] = r.get('pooled_bh_sharpe')
        flat['pooled_bh_roi'] = r.get('pooled_bh_roi')
        flat['mean_test_roi'] = r.get('mean_test_roi')
        flat['mean_test_profit_factor'] = r.get('mean_test_profit_factor')
        flat['mean_test_win_rate'] = r.get('mean_test_win_rate')
        flat['mean_test_trades'] = r.get('mean_test_trades')
        rows.append(flat)

    df_out = pd.DataFrame(rows)
    # ensure expected columns exist so downstream sorting/printing won't KeyError
    grid_keys = list(CONFIG.get('grid', {}).keys())
    metric_cols = ['mean_test_sharpe', 'pooled_test_sharpe', 'pooled_test_roi', 'pooled_bh_sharpe', 'pooled_bh_roi', 'mean_test_roi', 'mean_test_profit_factor', 'mean_test_win_rate', 'mean_test_trades']
    required = ['coin', 'timeframe', 'futures'] + grid_keys + metric_cols
    for c in required:
        if c not in df_out.columns:
            df_out[c] = np.nan
    # Save parquet as a machine-friendly backup as well
    try:
        df_out.to_parquet(out_path, index=False)
    except Exception:
        pass
    csv_path = os.path.join(CONFIG['results_dir'], f'grid_search_results_{ts}.csv')
    df_out.to_csv(csv_path, index=False)
    print('Saved results to', csv_path)

    # Plot pooled equity curves for top-N combos (by pooled_test_sharpe if available, else mean_test_sharpe)
    try:
        top_n = int(CONFIG.get('plot_top_n', 0))
        if top_n > 0 and results:
            # filter results with pooled equity
            plotable = [r for r in results if r.get('combo') is not None and (r.get('pooled_test_equity') is not None or r.get('pooled_bh_equity') is not None)]
            if plotable:
                # prefer pooled_test_sharpe as ranking key
                def _score(x):
                    v = x.get('pooled_test_sharpe')
                    if v is None:
                        v = x.get('mean_test_sharpe', 0.0)
                    return v

                plotable_sorted = sorted(plotable, key=_score, reverse=True)
                plots_dir = os.path.join(CONFIG['results_dir'], 'plots', ts)
                os.makedirs(plots_dir, exist_ok=True)
                for ix, r in enumerate(plotable_sorted[:top_n]):
                    try:
                        pooled = r.get('pooled_test_equity')
                        bh = r.get('pooled_bh_equity')
                        if (pooled is None or pooled.empty) and (bh is None or bh.empty):
                            continue
                        plt.figure(figsize=(10,6))
                        if pooled is not None and not pooled.empty:
                            edf = pooled.sort_values('time').reset_index(drop=True)
                            edf['dt'] = pd.to_datetime(edf['time'], unit='ms')
                            plt.plot(edf['dt'], edf['equity'], label='Strategy Equity')
                        if bh is not None and not bh.empty:
                            bdf = bh.sort_values('time').reset_index(drop=True)
                            bdf['dt'] = pd.to_datetime(bdf['time'], unit='ms')
                            plt.plot(bdf['dt'], bdf['equity'], label='Buy & Hold Equity')
                        plt.legend()
                        plt.title(f"Combo {ix+1} - coin={r.get('coin')} tf={r.get('timeframe')} futures={r.get('futures')}")
                        plt.xlabel('Time')
                        plt.ylabel('Equity (USD)')
                        # annotate metrics in subtitle
                        m1 = r.get('pooled_test_sharpe')
                        m2 = r.get('pooled_test_roi')
                        m3 = r.get('pooled_bh_sharpe')
                        m4 = r.get('pooled_bh_roi')
                        ann = f"strategy_sharpe={m1:.2f} roi={m2:.2f}% | bh_sharpe={m3:.2f} roi={m4:.2f}%" if m1 is not None else ''
                        plt.suptitle(ann, fontsize=9)
                        fname = os.path.join(plots_dir, f"combo_{ix+1}_{r.get('coin')}_{r.get('timeframe')}_{'futures' if r.get('futures') else 'spot'}_{ix+1}.png")
                        plt.tight_layout(rect=[0,0.03,1,0.95])
                        plt.savefig(fname)
                        plt.close()
                    except Exception as e:
                        print('Warning: failed to plot combo', r.get('combo'), e)
    except Exception as e:
        print('Warning: plotting failed', e)
    return df_out


if __name__ == '__main__':
    # quick smoke test: run a small sample to verify
    random.seed(CONFIG['random_seed'])
    print('Starting grid search smoke test...')
    df = run_grid_search(max_combinations=200)
    print('Top results (by mean_test_sharpe):')
    print(df.sort_values('mean_test_sharpe', ascending=False).head(10))
